from fingerprintgenerator import FingerprintGenerator
from nlp_module import run_nlp
from news_normalizer import NewsNormalizer
from collections import defaultdict
import os
import pandas as pd

def load_news_from_csv(csv_path, sample_frac=0.4): # porcentagem modificavel para execuÃ§Ã£o mais facil
    """Carrega notÃ­cias de um CSV"""
    try:
        print(f"ğŸ“– Lendo {csv_path}...")
        df = pd.read_csv(csv_path)
        sampled_df = df.sample(frac=sample_frac, random_state=42)
        
        texts = []
        filenames = []
        for idx, row in sampled_df.iterrows():
            combined_text = f"{row['title']} {row['text']}"
            texts.append(combined_text)
            filenames.append(f"{os.path.basename(csv_path)}_{idx}")
        
        print(f"   âœ… Carregadas {len(texts)} notÃ­cias de {csv_path}")
        return texts, filenames
    except Exception as e:
        print(f"âš ï¸ Erro ao carregar {csv_path}: {e}")
        return [], []

def main():
    true_path = "True.csv"
    fake_path = "Fake.csv"

    # Verificar se os arquivos existem
    print("ğŸ” Verificando arquivos...")
    if not os.path.exists(true_path):
        print(f"âŒ Arquivo {true_path} nÃ£o encontrado!")
        print("   Certifique-se de que True.csv estÃ¡ na pasta raiz")
        return
    if not os.path.exists(fake_path):
        print(f"âŒ Arquivo {fake_path} nÃ£o encontrado!")
        print("   Certifique-se de que Fake.csv estÃ¡ na pasta raiz")
        return

    print("âœ… Arquivos encontrados!")

    # Carregar dados
    print("\nğŸ“‚ Carregando notÃ­cias...")
    true_texts, true_filenames = load_news_from_csv(true_path, 0.4)
    fake_texts, fake_filenames = load_news_from_csv(fake_path, 0.4)

    texts = true_texts + fake_texts
    filenames = true_filenames + fake_filenames

    print(f"\nğŸ“Š Total de arquivos carregados: {len(texts)}")

    if len(texts) == 0:
        print("âŒ Nenhuma notÃ­cia carregada.")
        print("   Verifique se os arquivos CSV tÃªm dados vÃ¡lidos")
        return

    # NormalizaÃ§Ã£o
    print("\nğŸ”„ Normalizando textos...")
    normalizer = NewsNormalizer()
    normalized_texts = [normalizer.normalize_news(t) for t in texts]
    
    # Gerar fingerprints
    print("ğŸ”‘ Gerando fingerprints...")
    fp_gen = FingerprintGenerator(hash_sizes=[64])
    fingerprints = [fp_gen.generate_simhash(t, 64) for t in normalized_texts]
    
    # Comparar fingerprints (opcional)
    print("ğŸ“ Calculando distÃ¢ncias...")
    from itertools import combinations
    distances = []
    for (i, f1), (j, f2) in combinations(enumerate(fingerprints), 2):
        distance = bin(f1 ^ f2).count("1")
        distances.append(distance)
    
    if distances:
        avg_distance = sum(distances) / len(distances)
        print(f"   DistÃ¢ncia mÃ©dia de Hamming: {avg_distance:.2f}")

    # ClassificaÃ§Ã£o
    print("\nğŸ” Classificando notÃ­cias...")
    nlp_results = [run_nlp(t) for t in texts]

    # Agrupar resultados por tipo (True/Fake)
    results_by_type = defaultdict(list)
    for filename, (pred, conf, used_hamming) in zip(filenames, nlp_results):
        file_type = "True" if "True" in filename else "Fake"
        results_by_type[file_type].append((filename, pred, conf, used_hamming))

    # Determinar tamanho mÃ¡ximo do nome do arquivo
    max_len = max((len(file) for file in filenames), default=0)

    # Exibir resultados
    print("\nğŸ“Š RESULTADOS DA CLASSIFICAÃ‡ÃƒO:")
    for file_type, files in results_by_type.items():
        print(f"\nğŸ“‚ {file_type} News")
        for file, pred, conf, used_hamming in files:
            symbol = "âŒ" if pred == 1 else "âœ…"
            pred_text = "FAKE" if pred == 1 else "TRUE"
            
            if used_hamming:
                print(f"\tğŸ“„ {file:<{max_len}} --> {symbol} {pred_text} *H")
            else:
                print(f"\tğŸ“„ {file:<{max_len}} --> {symbol} {pred_text} (ConfianÃ§a: {conf:6.2%})")

    # EstatÃ­sticas gerais
    total_files = len(texts)
    fake_count = sum(1 for pred, _, _ in nlp_results if pred == 1)
    true_count = total_files - fake_count
    
    print(f"\nğŸ“ˆ ESTATÃSTICAS GERAIS:")
    print(f"Total de notÃ­cias analisadas: {total_files}")
    print(f"NotÃ­cias classificadas como TRUE: {true_count} ({true_count/total_files*100:.2f}%)")
    print(f"NotÃ­cias classificadas como FAKE: {fake_count} ({fake_count/total_files*100:.2f}%)")

    # Verificar acurÃ¡cia bÃ¡sica (se sabemos a verdade real)
    correct_predictions = 0
    for filename, (pred, _, _) in zip(filenames, nlp_results):
        true_label = 1 if "Fake" in filename else 0
        if pred == true_label:
            correct_predictions += 1
    
    accuracy = correct_predictions / total_files if total_files > 0 else 0
    print(f"ğŸ¯ AcurÃ¡cia geral: {accuracy:.2%}")

if __name__ == "__main__":
    main()